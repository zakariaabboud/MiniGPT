{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Mar 11 10:39:57 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 570.124.04             Driver Version: 570.124.04     CUDA Version: 12.8     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA RTX 4000 Ada Gene...    Off |   00000000:01:00.0 Off |                  Off |\n",
      "| 30%   40C    P8             17W /  130W |     149MiB /  20475MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A          399347      G   /usr/libexec/Xorg                        94MiB |\n",
      "|    0   N/A  N/A          399373      G   /usr/bin/gnome-shell                     18MiB |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/eleves-a/2022/brahim.touayouch/projects/MiniGPT/.venv/lib64/python3.9/site-packages/IPython/core/magics/osm.py:417: UserWarning: using dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/users/eleves-a/2022/brahim.touayouch/projects/MiniGPT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/eleves-a/2022/brahim.touayouch/projects/MiniGPT/.venv/lib64/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "%cd ..\n",
    "\n",
    "from mingpt.bpe import BPETokenizer\n",
    "from mingpt.model import GPT\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StoryDataset:\n",
    "    def __init__(self, data, tokenizer, block_size=128):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.block_size = block_size\n",
    "        self.data1 = [\n",
    "            self.format_example(data_point[\"text\"]) for data_point in data\n",
    "        ]\n",
    "\n",
    "    def format_example(self, text):\n",
    "        tokens = self.tokenizer(text, return_tensors='pt', padding=True, truncation=True, max_length=self.block_size)\n",
    "        tokens = tokens.squeeze(0)\n",
    "        \n",
    "        return tokens\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data1)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "            x (torch.Tensor): Input tokens (question + answer prompt).\n",
    "            y (torch.Tensor): Target tokens (shifted output).\n",
    "        \"\"\"\n",
    "        if isinstance(idx, int):\n",
    "            tokens = self.data1[idx]\n",
    "            x = tokens[:-1].clone().detach()\n",
    "            y = tokens[1:].clone().detach()\n",
    "            return x, y\n",
    "        \n",
    "        elif isinstance(idx, list) or isinstance(idx, torch.Tensor):\n",
    "            batch_tokens = [self.data1[i] for i in idx]\n",
    "            x_batch = [tokens[:-1].clone().detach() for tokens in batch_tokens]\n",
    "            y_batch = [tokens[1:].clone().detach() for tokens in batch_tokens]\n",
    "            return x_batch, y_batch\n",
    "        \n",
    "        else:\n",
    "            raise TypeError(f\"Invalid index type: {type(idx)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_dataset('roneneldan/TinyStories')\n",
    "data_frame = pd.DataFrame(data[\"train\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One day, a little girl named Lily found a need...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Once upon a time, there was a little car named...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>One day, a little fish named Fin was swimming ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Once upon a time, in a land full of trees, the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Once upon a time, there was a little girl name...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2119714</th>\n",
       "      <td>Once upon a time, in a small town, there lived...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2119715</th>\n",
       "      <td>Once upon a time, there was a little boy named...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2119716</th>\n",
       "      <td>Once upon a time, there was a big tree. Under ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2119717</th>\n",
       "      <td>Once upon a time, there was a little girl name...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2119718</th>\n",
       "      <td>Once upon a time, there was an adorable little...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2119719 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      text\n",
       "0        One day, a little girl named Lily found a need...\n",
       "1        Once upon a time, there was a little car named...\n",
       "2        One day, a little fish named Fin was swimming ...\n",
       "3        Once upon a time, in a land full of trees, the...\n",
       "4        Once upon a time, there was a little girl name...\n",
       "...                                                    ...\n",
       "2119714  Once upon a time, in a small town, there lived...\n",
       "2119715  Once upon a time, there was a little boy named...\n",
       "2119716  Once upon a time, there was a big tree. Under ...\n",
       "2119717  Once upon a time, there was a little girl name...\n",
       "2119718  Once upon a time, there was an adorable little...\n",
       "\n",
       "[2119719 rows x 1 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'One day, a little girl named Lily found a needle in her room. She knew it was difficult to play with it because it was sharp. Lily wanted to share the needle with her mom, so she could sew a button on her shirt.\\n\\nLily went to her mom and said, \"Mom, I found this needle. Can you share it with me and sew my shirt?\" Her mom smiled and said, \"Yes, Lily, we can share the needle and fix your shirt.\"\\n\\nTogether, they shared the needle and sewed the button on Lily\\'s shirt. It was not difficult for them because they were sharing and helping each other. After they finished, Lily thanked her mom for sharing the needle and fixing her shirt. They both felt happy because they had shared and worked together.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_frame.iloc[0]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 211971, Validation size: 953874, Test size: 953874\n"
     ]
    }
   ],
   "source": [
    "train_testvalid = data[\"train\"].train_test_split(test_size=0.9, seed=42)\n",
    "\n",
    "test_valid = train_testvalid[\"test\"].train_test_split(test_size=0.5, seed=42)\n",
    "\n",
    "train_data = train_testvalid[\"train\"]\n",
    "val_data = test_valid[\"train\"]\n",
    "test_data = test_valid[\"test\"]\n",
    "\n",
    "print(f\"Train size: {len(train_data)}, Validation size: {len(val_data)}, Test size: {len(test_data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BPETokenizer()\n",
    "\n",
    "train_dataset = StoryDataset(train_data.select(range(8000)), tokenizer=tokenizer, block_size=256)\n",
    "test_dataset = StoryDataset(test_data.select(range(4000)), tokenizer=tokenizer, block_size=256)\n",
    "val_dataset = StoryDataset(val_data.select(range(3000)), tokenizer=tokenizer, block_size=256)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 124.44M\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPT(\n",
       "  (transformer): ModuleDict(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): CausalSelfAttention(\n",
       "          (c_attn): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (c_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): ModuleDict(\n",
       "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (act): NewGELU()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_type = 'gpt2'\n",
    "device = 'cuda'\n",
    "\n",
    "model = GPT.from_pretrained(model_type)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(prompt='', num_samples=1, steps=20, do_sample=True):\n",
    "    tokenizer = BPETokenizer()\n",
    "\n",
    "    if prompt == '':\n",
    "        x = torch.tensor([[tokenizer.encoder.encoder['<|endoftext|>']]], dtype=torch.long)\n",
    "    else:\n",
    "        x = tokenizer(prompt, return_tensors='pt', padding=True, truncation=True, max_length=len(prompt.split())).to(device)\n",
    "\n",
    "    x = x.expand(num_samples, -1)\n",
    "\n",
    "    y = model.generate(x, max_new_tokens=steps, do_sample=do_sample, top_k=40)\n",
    "\n",
    "    endoftext_token_id = tokenizer.encoder.encoder['<|endoftext|>']\n",
    "\n",
    "    for i in range(num_samples):\n",
    "        out = tokenizer.decode(y[i].cpu().squeeze())\n",
    "\n",
    "        out_tokens = y[i].cpu().squeeze().tolist()\n",
    "        \n",
    "        if endoftext_token_id in out_tokens:\n",
    "            end_pos = out_tokens.index(endoftext_token_id)\n",
    "            out = tokenizer.decode(y[i].cpu().squeeze()[:end_pos])\n",
    "\n",
    "        print('\\n' + '-' * 80)\n",
    "        print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generation Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------------------------------------------------------------\n",
      "One day, a little girl named Lily found her feet in a pool with no money.\n",
      "\n",
      "\"It was because I said things like, 'Don't tell this to them or anyone, I'm sure you would get in trouble,\" he remembers.\n",
      "\n",
      "\"It happened last week. I knew it was happening to this girl and her story was good. It was a few days ago but it was very good.\"\n",
      "\n",
      "Lily was taken seriously after her story hit the news.\n",
      "\n",
      "\"That girl took care of herself as\n"
     ]
    }
   ],
   "source": [
    "generate(prompt='One day, a little girl named Lily found a', num_samples=1, steps=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------------------------------------------------------------\n",
      "One day. The week before.\n",
      "\n",
      "That's it.\n",
      "\n",
      "A day\n",
      "\n",
      "When I'm out\n",
      "\n",
      "That's it.\n",
      "\n",
      "\n",
      "I want to make a video\n",
      "\n",
      "A moment.\n",
      "\n",
      "A moment\n",
      "\n",
      "A moment\n",
      "\n",
      "How I like\n",
      "\n",
      "What I like.\n",
      "\n",
      "How I like.\n",
      "\n",
      "How much I like.\n",
      "\n",
      "I love\n",
      "\n",
      "Who's the best.\n",
      "\n",
      "What I like?\n",
      "\n",
      "Who's that.\n",
      "\n",
      "What I\n"
     ]
    }
   ],
   "source": [
    "generate(prompt='One day,', num_samples=1, steps=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Once upon a time, it is not a bad idea to send to the first woman to her new. I have sent two boys to the first time but was told of him last time the first time. I think it's best I do things to be as honest with him and be as honest as to what will happen to him. I feel like he is having a bad time but that is the time that he should be. My boys are trying so I believe that. I feel we are trying. The time is\n"
     ]
    }
   ],
   "source": [
    "generate(prompt='Once upon a time', num_samples=1, steps=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Loss and Perplexity Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate(model, dataset, partition, batch_size=24, device='cuda'):\n",
    "    model.eval()\n",
    "    test_loader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    total_loss = 0.0\n",
    "    num_batches = 0\n",
    "\n",
    "    for batch in test_loader:\n",
    "        batch = [t.to(device) for t in batch]\n",
    "        x, y = batch\n",
    "        _, loss = model(x, y)\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        num_batches += 1\n",
    "\n",
    "    avg_loss = total_loss / num_batches\n",
    "    perplexity = torch.exp(torch.tensor(avg_loss))\n",
    "    print(f\"{partition} Loss: {avg_loss:.4f}\")\n",
    "    print(f\"{partition} Perplexity: {perplexity:.4f}\")\n",
    "    return avg_loss, perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 5.1534\n",
      "Train Perplexity: 173.0197\n"
     ]
    }
   ],
   "source": [
    "train_loss, perplexity = evaluate(model=model, dataset=train_dataset, partition='Train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 5.0995\n",
      "Validation Perplexity: 163.9380\n"
     ]
    }
   ],
   "source": [
    "val_loss, perplexity = evaluate(model=model, dataset=val_dataset, partition='Validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 5.1494\n",
      "Test Perplexity: 172.3307\n"
     ]
    }
   ],
   "source": [
    "test_loss, perplexity = evaluate(model=model, dataset=test_dataset, partition='Test')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
