{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Mar 11 10:22:11 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 570.124.04             Driver Version: 570.124.04     CUDA Version: 12.8     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA RTX 4000 Ada Gene...    Off |   00000000:01:00.0 Off |                  Off |\n",
      "| 30%   29C    P8             11W /  130W |     149MiB /  20475MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A          399347      G   /usr/libexec/Xorg                        94MiB |\n",
      "|    0   N/A  N/A          399373      G   /usr/bin/gnome-shell                     18MiB |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/eleves-a/2022/brahim.touayouch/projects/MiniGPT/.venv/lib64/python3.9/site-packages/IPython/core/magics/osm.py:417: UserWarning: using dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/users/eleves-a/2022/brahim.touayouch/projects/MiniGPT\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/eleves-a/2022/brahim.touayouch/projects/MiniGPT/.venv/lib64/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "%cd ..\n",
    "\n",
    "from mingpt.bpe import BPETokenizer\n",
    "from mingpt.model import GPT\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StoryDataset:\n",
    "    def __init__(self, data, tokenizer, block_size=128):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.block_size = block_size\n",
    "        self.data1 = [\n",
    "            self.format_example(data_point[\"text\"]) for data_point in data\n",
    "        ]\n",
    "\n",
    "    def format_example(self, text):\n",
    "        tokens = self.tokenizer(text, return_tensors='pt', padding=True, truncation=True, max_length=self.block_size)\n",
    "        tokens = tokens.squeeze(0)\n",
    "        \n",
    "        return tokens\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data1)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "            x (torch.Tensor): Input tokens (question + answer prompt).\n",
    "            y (torch.Tensor): Target tokens (shifted output).\n",
    "        \"\"\"\n",
    "        if isinstance(idx, int):\n",
    "            tokens = self.data1[idx]\n",
    "            x = tokens[:-1].clone().detach()\n",
    "            y = tokens[1:].clone().detach()\n",
    "            return x, y\n",
    "        \n",
    "        elif isinstance(idx, list) or isinstance(idx, torch.Tensor):\n",
    "            batch_tokens = [self.data1[i] for i in idx]\n",
    "            x_batch = [tokens[:-1].clone().detach() for tokens in batch_tokens]\n",
    "            y_batch = [tokens[1:].clone().detach() for tokens in batch_tokens]\n",
    "            return x_batch, y_batch\n",
    "        \n",
    "        else:\n",
    "            raise TypeError(f\"Invalid index type: {type(idx)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_dataset('roneneldan/TinyStories')\n",
    "data_frame = pd.DataFrame(data[\"train\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One day, a little girl named Lily found a need...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Once upon a time, there was a little car named...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>One day, a little fish named Fin was swimming ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Once upon a time, in a land full of trees, the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Once upon a time, there was a little girl name...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2119714</th>\n",
       "      <td>Once upon a time, in a small town, there lived...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2119715</th>\n",
       "      <td>Once upon a time, there was a little boy named...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2119716</th>\n",
       "      <td>Once upon a time, there was a big tree. Under ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2119717</th>\n",
       "      <td>Once upon a time, there was a little girl name...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2119718</th>\n",
       "      <td>Once upon a time, there was an adorable little...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2119719 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                      text\n",
       "0        One day, a little girl named Lily found a need...\n",
       "1        Once upon a time, there was a little car named...\n",
       "2        One day, a little fish named Fin was swimming ...\n",
       "3        Once upon a time, in a land full of trees, the...\n",
       "4        Once upon a time, there was a little girl name...\n",
       "...                                                    ...\n",
       "2119714  Once upon a time, in a small town, there lived...\n",
       "2119715  Once upon a time, there was a little boy named...\n",
       "2119716  Once upon a time, there was a big tree. Under ...\n",
       "2119717  Once upon a time, there was a little girl name...\n",
       "2119718  Once upon a time, there was an adorable little...\n",
       "\n",
       "[2119719 rows x 1 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'One day, a little girl named Lily found a needle in her room. She knew it was difficult to play with it because it was sharp. Lily wanted to share the needle with her mom, so she could sew a button on her shirt.\\n\\nLily went to her mom and said, \"Mom, I found this needle. Can you share it with me and sew my shirt?\" Her mom smiled and said, \"Yes, Lily, we can share the needle and fix your shirt.\"\\n\\nTogether, they shared the needle and sewed the button on Lily\\'s shirt. It was not difficult for them because they were sharing and helping each other. After they finished, Lily thanked her mom for sharing the needle and fixing her shirt. They both felt happy because they had shared and worked together.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_frame.iloc[0]['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 211971, Validation size: 953874, Test size: 953874\n"
     ]
    }
   ],
   "source": [
    "train_testvalid = data[\"train\"].train_test_split(test_size=0.9, seed=42)\n",
    "\n",
    "test_valid = train_testvalid[\"test\"].train_test_split(test_size=0.5, seed=42)\n",
    "\n",
    "train_data = train_testvalid[\"train\"]\n",
    "val_data = test_valid[\"train\"]\n",
    "test_data = test_valid[\"test\"]\n",
    "\n",
    "print(f\"Train size: {len(train_data)}, Validation size: {len(val_data)}, Test size: {len(test_data)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BPETokenizer()\n",
    "\n",
    "train_dataset = StoryDataset(train_data.select(range(8000)), tokenizer=tokenizer, block_size=256)\n",
    "test_dataset = StoryDataset(test_data.select(range(4000)), tokenizer=tokenizer, block_size=256)\n",
    "val_dataset = StoryDataset(val_data.select(range(3000)), tokenizer=tokenizer, block_size=256)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 124.44M\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPT(\n",
       "  (transformer): ModuleDict(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): CausalSelfAttention(\n",
       "          (c_attn): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (c_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): ModuleDict(\n",
       "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (act): NewGELU()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_type = 'gpt2'\n",
    "device = 'cuda'\n",
    "\n",
    "model = GPT.load_model_from_saved(model_type, \"./saved_model\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(prompt='', num_samples=1, steps=20, do_sample=True):\n",
    "    tokenizer = BPETokenizer()\n",
    "\n",
    "    if prompt == '':\n",
    "        x = torch.tensor([[tokenizer.encoder.encoder['<|endoftext|>']]], dtype=torch.long)\n",
    "    else:\n",
    "        x = tokenizer(prompt, return_tensors='pt', padding=True, truncation=True, max_length=len(prompt.split())).to(device)\n",
    "\n",
    "    x = x.expand(num_samples, -1)\n",
    "\n",
    "    y = model.generate(x, max_new_tokens=steps, do_sample=do_sample, top_k=40)\n",
    "\n",
    "    endoftext_token_id = tokenizer.encoder.encoder['<|endoftext|>']\n",
    "\n",
    "    for i in range(num_samples):\n",
    "        out = tokenizer.decode(y[i].cpu().squeeze())\n",
    "\n",
    "        out_tokens = y[i].cpu().squeeze().tolist()\n",
    "        \n",
    "        if endoftext_token_id in out_tokens:\n",
    "            end_pos = out_tokens.index(endoftext_token_id)\n",
    "            out = tokenizer.decode(y[i].cpu().squeeze()[:end_pos])\n",
    "\n",
    "        print('\\n' + '-' * 80)\n",
    "        print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generation Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------------------------------------------------------------\n",
      "One day, a little girl named Lily found a red dress. She liked to wear it everyday, and soon the dresses started to sparkle. She wore them all day long, but after that she forgot about the dress.\n",
      "\n",
      "When Lily saw where her dress was going, she got so excited. She ran over to her mom but she couldn't find the dress. Then she started to cry.\n",
      "\n",
      "When Lily looked out of the dress, she saw her friends in the garden who were playing. It felt like a great day to\n"
     ]
    }
   ],
   "source": [
    "generate(prompt='One day, a little girl named Lily found a', num_samples=1, steps=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------------------------------------------------------------\n",
      "One day, a small girl found a piece of whiteboard. She decided to take the whiteboard with her.\n",
      "\n",
      "She got her pieces of white board and started to glue it together. She decided that it was the best idea to go with a painter\n"
     ]
    }
   ],
   "source": [
    "generate(prompt='One day,', num_samples=1, steps=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Once upon a time there was a boy named Timmy. Timmy had a big green dog named Spanky. He loved to play outside with Spanky and their friends. One day, when Timmy's friends were playing, they saw Spanky getting chased down a\n"
     ]
    }
   ],
   "source": [
    "generate(prompt='Once upon a time', num_samples=1, steps=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Loss and Perplexity Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate(model, dataset, partition, batch_size=24, device='cuda'):\n",
    "    model.eval()\n",
    "    test_loader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    total_loss = 0.0\n",
    "    num_batches = 0\n",
    "\n",
    "    for batch in test_loader:\n",
    "        batch = [t.to(device) for t in batch]\n",
    "        x, y = batch\n",
    "        _, loss = model(x, y)\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        num_batches += 1\n",
    "\n",
    "    avg_loss = total_loss / num_batches\n",
    "    perplexity = torch.exp(torch.tensor(avg_loss))\n",
    "    print(f\"{partition} Loss: {avg_loss:.4f}\")\n",
    "    print(f\"{partition} Perplexity: {perplexity:.4f}\")\n",
    "    return avg_loss, perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.2712\n",
      "Train Perplexity: 3.5650\n"
     ]
    }
   ],
   "source": [
    "train_loss, perplexity = evaluate(model=model, dataset=train_dataset, partition='Train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 1.3452\n",
      "Validation Perplexity: 3.8389\n"
     ]
    }
   ],
   "source": [
    "val_loss, perplexity = evaluate(model=model, dataset=val_dataset, partition='Validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 1.3397\n",
      "Perplexity: 3.8180\n"
     ]
    }
   ],
   "source": [
    "test_loss, perplexity = evaluate(model=model, dataset=test_dataset, partition='Test')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
