{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install regex requests torch numpy transformers datasets evaluate rouge_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tue Mar  4 10:23:02 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 570.124.04             Driver Version: 570.124.04     CUDA Version: 12.8     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  NVIDIA RTX 4000 Ada Gene...    Off |   00000000:01:00.0 Off |                  Off |\n",
      "| 30%   38C    P2             19W /  130W |   19637MiB /  20475MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|    0   N/A  N/A         1365470      G   /usr/libexec/Xorg                        94MiB |\n",
      "|    0   N/A  N/A         1365496      G   /usr/bin/gnome-shell                     18MiB |\n",
      "|    0   N/A  N/A         1542559      C   ...ria.abboud/my_venv/bin/python      19480MiB |\n",
      "+-----------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mingpt.bpe import BPETokenizer\n",
    "from mingpt.model import GPT\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "from evaluate import load"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QADataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, block_size=128):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.block_size = block_size\n",
    "        self.data = [\n",
    "            self.format_example(data_point[\"instruction\"], data_point[\"demonstration\"]) for data_point in data\n",
    "        ]\n",
    "\n",
    "    def format_example(self, question, answer):\n",
    "        text = f\"<|human|>: {question} \\n <|assistant|>: {answer} <|endoftext|>\"\n",
    "        tokens = self.tokenizer(text)\n",
    "        tokens = tokens.squeeze(0).tolist()[:self.block_size]        \n",
    "        prompt = text.split(\"<|assistant|>:\")[0] + \"<|assistant|>:\"\n",
    "        self.assistant_index = len(self.tokenizer(prompt).squeeze(0).tolist())\n",
    "\n",
    "        return tokens\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "            x (torch.Tensor): Input tokens (question + answer prompt).\n",
    "            y (torch.Tensor): Target tokens (shifted output).\n",
    "        \"\"\"\n",
    "        tokens = self.data[idx]\n",
    "        x = torch.tensor(tokens[:-1], dtype=torch.long)  # Exclude last token for input\n",
    "        y = torch.tensor(tokens[1:], dtype=torch.long)   # Exclude first token for output\n",
    "\n",
    "        y[:self.assistant_index] = -1  # Mask loss for assistant tokens\n",
    "        \n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>instruction</th>\n",
       "      <th>demonstration</th>\n",
       "      <th>meta</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hi, I want to learn to play horseshoes. Can yo...</td>\n",
       "      <td>I can, but maybe I should begin by telling you...</td>\n",
       "      <td>{'source': 'helpful-anthropic-raw'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>How do I teach kids to meditate?</td>\n",
       "      <td>Great question! That’s a really useful skill t...</td>\n",
       "      <td>{'source': 'helpful-anthropic-raw'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Can you tell me the steps for getting a harbor...</td>\n",
       "      <td>Sure. I believe you’ll need a copy of the mari...</td>\n",
       "      <td>{'source': 'helpful-anthropic-raw'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>How can I store food if I don't have a pantry?</td>\n",
       "      <td>You could store the food in a refrigerator, th...</td>\n",
       "      <td>{'source': 'helpful-anthropic-raw'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>what are some good novels for a 9 year old?</td>\n",
       "      <td>That depends on the 9 year old, but if they li...</td>\n",
       "      <td>{'source': 'helpful-anthropic-raw'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147701</th>\n",
       "      <td>Given the following sentence, classify it into...</td>\n",
       "      <td>Fact</td>\n",
       "      <td>{'source': 'helpful-self-instruct-raw'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147702</th>\n",
       "      <td>A person wants to write a book. he/she writes ...</td>\n",
       "      <td>Chapter 1 - The History of China\\nChapter 2 - ...</td>\n",
       "      <td>{'source': 'helpful-self-instruct-raw'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147703</th>\n",
       "      <td>Tell me how you would make a popular app game.</td>\n",
       "      <td>I would make a game that is similar to 2048. T...</td>\n",
       "      <td>{'source': 'helpful-self-instruct-raw'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147704</th>\n",
       "      <td>Describe your dream house to me.\\n\\nOutput:</td>\n",
       "      <td>My dream house is a two-story building with a ...</td>\n",
       "      <td>{'source': 'helpful-self-instruct-raw'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147705</th>\n",
       "      <td>Task: Rewrite the given sentence using only on...</td>\n",
       "      <td>Dog</td>\n",
       "      <td>{'source': 'helpful-self-instruct-raw'}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>147706 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              instruction  \\\n",
       "0       Hi, I want to learn to play horseshoes. Can yo...   \n",
       "1                        How do I teach kids to meditate?   \n",
       "2       Can you tell me the steps for getting a harbor...   \n",
       "3          How can I store food if I don't have a pantry?   \n",
       "4             what are some good novels for a 9 year old?   \n",
       "...                                                   ...   \n",
       "147701  Given the following sentence, classify it into...   \n",
       "147702  A person wants to write a book. he/she writes ...   \n",
       "147703     Tell me how you would make a popular app game.   \n",
       "147704        Describe your dream house to me.\\n\\nOutput:   \n",
       "147705  Task: Rewrite the given sentence using only on...   \n",
       "\n",
       "                                            demonstration  \\\n",
       "0       I can, but maybe I should begin by telling you...   \n",
       "1       Great question! That’s a really useful skill t...   \n",
       "2       Sure. I believe you’ll need a copy of the mari...   \n",
       "3       You could store the food in a refrigerator, th...   \n",
       "4       That depends on the 9 year old, but if they li...   \n",
       "...                                                   ...   \n",
       "147701                                               Fact   \n",
       "147702  Chapter 1 - The History of China\\nChapter 2 - ...   \n",
       "147703  I would make a game that is similar to 2048. T...   \n",
       "147704  My dream house is a two-story building with a ...   \n",
       "147705                                                Dog   \n",
       "\n",
       "                                           meta  \n",
       "0           {'source': 'helpful-anthropic-raw'}  \n",
       "1           {'source': 'helpful-anthropic-raw'}  \n",
       "2           {'source': 'helpful-anthropic-raw'}  \n",
       "3           {'source': 'helpful-anthropic-raw'}  \n",
       "4           {'source': 'helpful-anthropic-raw'}  \n",
       "...                                         ...  \n",
       "147701  {'source': 'helpful-self-instruct-raw'}  \n",
       "147702  {'source': 'helpful-self-instruct-raw'}  \n",
       "147703  {'source': 'helpful-self-instruct-raw'}  \n",
       "147704  {'source': 'helpful-self-instruct-raw'}  \n",
       "147705  {'source': 'helpful-self-instruct-raw'}  \n",
       "\n",
       "[147706 rows x 3 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = load_dataset(\"HuggingFaceH4/helpful-instructions\")\n",
    "pd.DataFrame(data[\"train\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'instruction': 'Hi, I want to learn to play horseshoes. Can you teach me?',\n",
       " 'demonstration': 'I can, but maybe I should begin by telling you that a typical game consists of 2 players and 6 or 8 horseshoes.',\n",
       " 'meta': {'source': 'helpful-anthropic-raw'}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"train\"].select(range(4))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input tokens: tensor([   27,    91, 10734,    91, 31175,  1374,   466,   314,  4545,  3988,\n",
      "          284,  1117, 12027,    30,   220,   198,  1279,    91,   562, 10167,\n",
      "           91, 31175,  3878,  1808,     0,  1320,   447,   247,    82,   257,\n",
      "         1107,  4465,  5032,   284, 32237,    11,   340,   460,  2222,  4167,\n",
      "           11,  9480,    11,   290, 12157,    13,   314,   447,   247,    76,\n",
      "         9675,   345,   765,   284,  4545,   534,  3988,   546,   340,    13,\n",
      "         1279,    91,   437,  1659,  5239,    91])\n",
      "Output tokens: tensor([   -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,\n",
      "           -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,\n",
      "           -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,    -1,\n",
      "           -1,    -1,   284, 32237,    11,   340,   460,  2222,  4167,    11,\n",
      "         9480,    11,   290, 12157,    13,   314,   447,   247,    76,  9675,\n",
      "          345,   765,   284,  4545,   534,  3988,   546,   340,    13,  1279,\n",
      "           91,   437,  1659,  5239,    91,    29])\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BPETokenizer()\n",
    "\n",
    "train_dataset = QADataset(data[\"train\"], tokenizer=tokenizer, block_size=1024)\n",
    "\n",
    "x, y = train_dataset[1]\n",
    "print(f\"Input tokens: {x}\")\n",
    "print(f\"Output tokens: {y}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 124.44M\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPT(\n",
       "  (transformer): ModuleDict(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): CausalSelfAttention(\n",
       "          (c_attn): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (c_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): ModuleDict(\n",
       "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (act): NewGELU()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_type = 'gpt2'\n",
    "device = 'cuda'\n",
    "\n",
    "model = GPT.from_pretrained(model_type)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(prompt='', num_samples=1, steps=20, do_sample=True):\n",
    "\n",
    "    tokenizer = BPETokenizer()\n",
    "    if prompt == '':\n",
    "        x = torch.tensor([[tokenizer.encoder.encoder['<|endoftext|>']]], dtype=torch.long)\n",
    "    else:\n",
    "        x = tokenizer(prompt).to(device)\n",
    "\n",
    "    x = x.expand(num_samples, -1)\n",
    "\n",
    "    y = model.generate(x, max_new_tokens=steps, do_sample=do_sample, top_k=40)\n",
    "    \n",
    "    for i in range(num_samples):\n",
    "        out = tokenizer.decode(y[i].cpu().squeeze())\n",
    "        print('\\n'+'-'*80)\n",
    "        print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------------------------------------------------------------\n",
      "How do I teach kids to meditate?\n",
      "\n",
      "Dana is a psychologist and her PhD is in psychology at the same institution a long term. The focus on the problem is that there are many people who would say if I teach the student how to meditate, that would be one of\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "How do I teach kids to meditate? Are they smart people they can be better meditative practitioners? How do you teach them to meditate? By how do you do?\n",
      "\n",
      "\"This is not a matter of any one specific school,\" he says. \"This one school we're\n"
     ]
    }
   ],
   "source": [
    "generate(prompt='How do I teach kids to meditate?', num_samples=2, steps=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Finetuning With SFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running on device cuda\n"
     ]
    }
   ],
   "source": [
    "from mingpt.trainer import Trainer\n",
    "\n",
    "train_config = Trainer.get_default_config()\n",
    "train_config.learning_rate = 5e-4\n",
    "train_config.batch_size = 6\n",
    "train_config.max_iters = 200\n",
    "train_config.num_workers = 1\n",
    "trainer = Trainer(train_config, model, train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Progress: 100%|██████████| 200/200 [00:31<00:00,  6.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 199 / 200\n",
      "Loss: 1.007644\n",
      "Learning Rate: 5.000000e-04\n",
      "Learning Rate: 5.000000e-04\n",
      "Training finished!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "trainer.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT(\n",
       "  (transformer): ModuleDict(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): CausalSelfAttention(\n",
       "          (c_attn): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (c_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): ModuleDict(\n",
       "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (act): NewGELU()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------------------------------------------------------------\n",
      "How do I teach kids to meditate? \n",
      " <|assistant|>: Are you interested in meditation? <|endoftext|> <|> - Listen to meditation.\n",
      "- Go to a meditation practice. <|endoftext|> <|endoftext|\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "How do I teach kids to meditate? What is the best way to meditate? \n",
      " <|assistant|>: The best way to meditate is to practice meditation and meditation, then practice meditating to relieve yourself, and then practice meditating to manage the pressure to practice med\n"
     ]
    }
   ],
   "source": [
    "generate(prompt='How do I teach kids to meditate?', num_samples=2, steps=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model\n",
    "torch.save(model.state_dict(), 'models/SFT_finetuned.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the fine-tuned model\n",
    "model.load_state_dict(torch.load('models/SFT_finetuned.pth'))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BPETokenizer()\n",
    "\n",
    "def generate_evaluate(prompt='', num_samples=1, steps=20, do_sample=True):\n",
    "    if prompt == '':\n",
    "        x = torch.tensor([[tokenizer.encoder.encoder['<|endoftext|>']]], dtype=torch.long)\n",
    "    else:\n",
    "        x = tokenizer(prompt).to(device)\n",
    "\n",
    "    x = x.expand(num_samples, -1)\n",
    "\n",
    "    y = model.generate(x, max_new_tokens=steps, do_sample=do_sample, top_k=40)\n",
    "    \n",
    "    generated_responses = []\n",
    "    for i in range(num_samples):\n",
    "        out = tokenizer.decode(y[i].cpu().squeeze())\n",
    "        generated_responses.append(out)\n",
    "    \n",
    "    return generated_responses\n",
    "\n",
    "references = []\n",
    "predictions = []\n",
    "\n",
    "num_samples = 2\n",
    "\n",
    "for i, sample in enumerate(data[\"train\"].select(range(num_samples))):\n",
    "    prompt = sample[\"instruction\"]\n",
    "    reference = [sample[\"demonstration\"]]\n",
    "    generated_text = generate_evaluate(prompt=prompt, num_samples=1, steps=50, do_sample=True)[0]\n",
    "\n",
    "    references.append(reference)\n",
    "    predictions.append(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 BLEU Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU Score: {'bleu': 0.0, 'precisions': [0.125, 0.00909090909090909, 0.0, 0.0], 'brevity_penalty': 1.0, 'length_ratio': 1.9649122807017543, 'translation_length': 112, 'reference_length': 57}\n"
     ]
    }
   ],
   "source": [
    "bleu = load(\"bleu\")\n",
    "\n",
    "bleu_score = bleu.compute(predictions=predictions, references=references)\n",
    "print(\"BLEU Score:\", bleu_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Rouge Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUGE Scores: {'rouge1': np.float64(0.16991107906753183), 'rouge2': np.float64(0.0), 'rougeL': np.float64(0.12112472963229992), 'rougeLsum': np.float64(0.15621244893054553)}\n"
     ]
    }
   ],
   "source": [
    "rouge = load(\"rouge\")\n",
    "\n",
    "scores = rouge.compute(predictions=predictions, references=references)\n",
    "print(\"ROUGE Scores:\", scores)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_venv",
   "language": "python",
   "name": "my_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
