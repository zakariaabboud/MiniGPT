{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install regex requests torch numpy transformers datasets evaluate rouge_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mingpt.bpe import BPETokenizer\n",
    "from mingpt.model import GPT\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "from evaluate import load"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QADataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, block_size=128):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.block_size = block_size\n",
    "        self.data = [\n",
    "            self.format_example(data_point[\"instruction\"], data_point[\"demonstration\"]) for data_point in data\n",
    "        ]\n",
    "\n",
    "    def format_example(self, question, answer):\n",
    "        text = f\"<|human|>: {question} \\n <|assistant|>: {answer} <|endoftext|>\"\n",
    "        tokens = self.tokenizer(text)\n",
    "        tokens = tokens.squeeze(0).tolist()[:self.block_size]        \n",
    "        prompt = text.split(\"<|assistant|>:\")[0] + \"<|assistant|>:\"\n",
    "        self.assistant_index = len(self.tokenizer(prompt).squeeze(0).tolist())\n",
    "\n",
    "        return tokens\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "            x (torch.Tensor): Input tokens (question + answer prompt).\n",
    "            y (torch.Tensor): Target tokens (shifted output).\n",
    "        \"\"\"\n",
    "        tokens = self.data[idx]\n",
    "        x = torch.tensor(tokens[:-1], dtype=torch.long)  # Exclude last token for input\n",
    "        y = torch.tensor(tokens[1:], dtype=torch.long)   # Exclude first token for output\n",
    "\n",
    "        y[:self.assistant_index] = -1  # Mask loss for assistant tokens\n",
    "        \n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_dataset(\"HuggingFaceH4/helpful-instructions\")\n",
    "pd.DataFrame(data[\"train\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"train\"].select(range(4))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BPETokenizer()\n",
    "\n",
    "train_dataset = QADataset(data[\"train\"], tokenizer=tokenizer, block_size=1024)\n",
    "\n",
    "x, y = train_dataset[1]\n",
    "print(f\"Input tokens: {x}\")\n",
    "print(f\"Output tokens: {y}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_type = 'gpt2'\n",
    "device = 'cuda'\n",
    "\n",
    "model = GPT.from_pretrained(model_type)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(prompt='', num_samples=1, steps=20, do_sample=True):\n",
    "\n",
    "    tokenizer = BPETokenizer()\n",
    "    if prompt == '':\n",
    "        x = torch.tensor([[tokenizer.encoder.encoder['<|endoftext|>']]], dtype=torch.long)\n",
    "    else:\n",
    "        x = tokenizer(prompt).to(device)\n",
    "\n",
    "    x = x.expand(num_samples, -1)\n",
    "\n",
    "    y = model.generate(x, max_new_tokens=steps, do_sample=do_sample, top_k=40)\n",
    "    \n",
    "    for i in range(num_samples):\n",
    "        out = tokenizer.decode(y[i].cpu().squeeze())\n",
    "        print('\\n'+'-'*80)\n",
    "        print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate(prompt='How do I teach kids to meditate?', num_samples=2, steps=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Finetuning With SFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mingpt.trainer import Trainer\n",
    "\n",
    "train_config = Trainer.get_default_config()\n",
    "train_config.learning_rate = 5e-4\n",
    "train_config.batch_size = 6\n",
    "train_config.max_iters = 200\n",
    "train_config.num_workers = 0\n",
    "trainer = Trainer(train_config, model, train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "def batch_end_callback(trainer):\n",
    "    if trainer.iter_num % 100 == 0:\n",
    "        print(f\"iter_dt {trainer.iter_dt * 1000:.2f}ms; iter {trainer.iter_num}: train loss {trainer.loss.item():.5f}\")\n",
    "        losses.append(trainer.loss.item())\n",
    "trainer.set_callback('on_batch_end', batch_end_callback)\n",
    "\n",
    "trainer.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate(prompt='How do I teach kids to meditate?', num_samples=2, steps=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model\n",
    "torch.save(model.state_dict(), 'models/SFT_finetuned.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the fine-tuned model\n",
    "model.load_state_dict(torch.load('models/SFT_finetuned.pth'))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BPETokenizer()\n",
    "\n",
    "def generate_evaluate(prompt='', num_samples=1, steps=20, do_sample=True):\n",
    "    if prompt == '':\n",
    "        x = torch.tensor([[tokenizer.encoder.encoder['<|endoftext|>']]], dtype=torch.long).to(device)\n",
    "    else:\n",
    "        x = tokenizer(prompt).to(device)\n",
    "\n",
    "    x = x.expand(num_samples, -1)\n",
    "\n",
    "    y = model.generate(x, max_new_tokens=steps, do_sample=do_sample, top_k=40)\n",
    "\n",
    "    generated_responses = []\n",
    "    for i in range(num_samples):\n",
    "        out = tokenizer.decode(y[i].cpu().squeeze())\n",
    "        generated_responses.append(out)\n",
    "    \n",
    "    return generated_responses\n",
    "\n",
    "references = []\n",
    "predictions = []\n",
    "\n",
    "num_samples = 500\n",
    "\n",
    "for i, sample in enumerate(data[\"train\"].select(range(num_samples))):\n",
    "    prompt = sample[\"instruction\"]\n",
    "    reference = [sample[\"demonstration\"]]\n",
    "    generated_text = generate_evaluate(prompt=prompt, num_samples=1, steps=50, do_sample=True)[0]\n",
    "\n",
    "    references.append(reference)\n",
    "    predictions.append(generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 BLEU Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bleu = load(\"bleu\")\n",
    "\n",
    "bleu_score = bleu.compute(predictions=predictions, references=references)\n",
    "print(\"BLEU Score:\", bleu_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Rouge Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge = load(\"rouge\")\n",
    "\n",
    "scores = rouge.compute(predictions=predictions, references=references)\n",
    "print(\"ROUGE Scores:\", scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_evaluate(\"What is AI ?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the fine-tuned model\n",
    "\n",
    "path = \"/Data/zakaria.abboud/SFT_finetuned.pth\"\n",
    "model_type = 'gpt2'\n",
    "device = 'cuda'\n",
    "\n",
    "model = GPT.from_pretrained(model_type)\n",
    "\n",
    "# model.load_state_dict(torch.load(path))\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
